{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/vision/token_learner/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 10:19:59.776334: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-12 10:19:59.824653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733995199.863678    4507 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733995199.877452    4507 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 10:19:59.985586: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from tensorflow import data as tf_data\n",
    "\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "BATCH_SIZE = 256\n",
    "AUTO = tf_data.AUTOTUNE\n",
    "INPUT_SHAPE = (32,32,3)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 10\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 48 # reize input images to this size\n",
    "PATCH_SIZE = 6 # Size of the patches to be extrachted from the input images. \n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE ) ** 2\n",
    "\n",
    "# VIT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "MLP_UNITS = [\n",
    "    PROJECTION_DIM * 2,\n",
    "    PROJECTION_DIM\n",
    "\n",
    "]\n",
    "\n",
    "# TOKENLEARNER\n",
    "NUM_TOKENS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prepare the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 40000\n",
      "Validation samples: 10000\n",
      "Testing samples: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733995202.202031    4507 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13572 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 dataset.\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_val, y_val) = (\n",
    "    (x_train[:40000], y_train[:40000]),\n",
    "    (x_train[40000:], y_train[40000:]),\n",
    ")\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_val)}\")\n",
    "print(f\"Testing samples: {len(x_test)}\")\n",
    "\n",
    "# Convert to tf.data.Dataset objects.\n",
    "train_ds = tf_data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(BATCH_SIZE * 100).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "val_ds = tf_data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "test_ds = tf_data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation\n",
    "- Rescaling\n",
    "- REsizing\n",
    "- Random cropping (fixed-sized or random sized)\n",
    "- Random horizontal flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(1/255.0),\n",
    "        layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n",
    "        layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        layers.RandomFlip(\"horizontal\")\n",
    "    ],\n",
    "    name=\"data_augmentation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional embedding module\n",
    "- multi-head self attention layers\n",
    "- fully-connected feed forward networks (MLP) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim = num_patches, output_dim=projection_dim\n",
    "        )\n",
    "    \n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start = 0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        encoded = patch + self.position_embedding(positions)\n",
    "        return encoded\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "MLP block for Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, dropout_rate, hidden_units):\n",
    "    # Iterate over the hidden units and\n",
    "    # add Dense => Dropout.\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=ops.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_learner(inputs, number_of_tokens=NUM_TOKENS):\n",
    "    # Layer normalize the inpus.\n",
    "    x = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(inputs)\n",
    "    # Applying Conv2D => Reshape => Permute\n",
    "    # The reshape and permute is done to help with the next steps of\n",
    "    # multiplication and Global Average Pooling.\n",
    "\n",
    "    attention_maps = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3,3),\n",
    "                activation=ops.gelu,\n",
    "                padding=\"same\",\n",
    "                use_bias=False\n",
    "            ),\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3,3),\n",
    "                activation=ops.gelu,\n",
    "                padding=\"same\",\n",
    "                use_bias=False               \n",
    "            ),\n",
    "            layers.Conv2D(\n",
    "                filters=number_of_tokens,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=ops.gelu,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "            ),\n",
    "            # Reshape and Permute\n",
    "            layers.Reshape((-1, number_of_tokens)),  # (B, H*W, num_of_tokens)\n",
    "            layers.Permute((2, 1)),\n",
    "        ]\n",
    "    )(\n",
    "        x\n",
    "    )  # (B, num_of_tokens, H*W)\n",
    "\n",
    "    # Reshape the input to align it with the output of the conv block.\n",
    "    num_filters = inputs.shape[-1]\n",
    "    inputs = layers.Reshape((1, -1, num_filters))(inputs)  # inputs == (B, 1, H*W, C)\n",
    "\n",
    "    # Element-Wise multiplication of the attention maps and the inputs\n",
    "    attended_inputs = (\n",
    "        ops.expand_dims(attention_maps, axis=-1) * inputs\n",
    "    )  # (B, num_tokens, H*W, C)\n",
    "\n",
    "    # Global average pooling the element wise multiplication result.\n",
    "    outputs = ops.mean(attended_inputs, axis=2)  # (B, num_tokens, C)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(encoded_patches):\n",
    "    # Layer normalization 1.\n",
    "    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
    "\n",
    "    # Multi Head Self Attention layer 1.\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "    )(x1, x1)\n",
    "\n",
    "    # Skip connection 1.\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "    # Layer normalization 2.\n",
    "    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n",
    "\n",
    "    # MLP layer 1.\n",
    "    x4 = mlp(x3, hidden_units=MLP_UNITS, dropout_rate=0.1)\n",
    "\n",
    "    # Skip connection 2.\n",
    "    encoded_patches = layers.Add()([x4, x2])\n",
    "    return encoded_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier(use_token_learner=True, token_learner_units=NUM_TOKENS):\n",
    "    inputs = layers.Input(shape=INPUT_SHAPE)  # (B, H, W, C)\n",
    "\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "\n",
    "    # Create patches and project the pathces.\n",
    "    projected_patches = layers.Conv2D(\n",
    "        filters=PROJECTION_DIM,\n",
    "        kernel_size=(PATCH_SIZE, PATCH_SIZE),\n",
    "        strides=(PATCH_SIZE, PATCH_SIZE),\n",
    "        padding=\"VALID\",\n",
    "    )(augmented)\n",
    "    _, h, w, c = projected_patches.shape\n",
    "    projected_patches = layers.Reshape((h * w, c))(\n",
    "        projected_patches\n",
    "    )  # (B, number_patches, projection_dim)\n",
    "\n",
    "    # Add positional embeddings to the projected patches.\n",
    "    encoded_patches = PatchEncoder(\n",
    "        num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM\n",
    "    )(\n",
    "        projected_patches\n",
    "    )  # (B, number_patches, projection_dim)\n",
    "    encoded_patches = layers.Dropout(0.1)(encoded_patches)\n",
    "\n",
    "    # Iterate over the number of layers and stack up blocks of\n",
    "    # Transformer.\n",
    "    for i in range(NUM_LAYERS):\n",
    "        # Add a Transformer block.\n",
    "        encoded_patches = transformer(encoded_patches)\n",
    "\n",
    "        # Add TokenLearner layer in the middle of the\n",
    "        # architecture. The paper suggests that anywhere\n",
    "        # between 1/2 or 3/4 will work well.\n",
    "        if use_token_learner and i == NUM_LAYERS // 2:\n",
    "            _, hh, c = encoded_patches.shape\n",
    "            h = int(math.sqrt(hh))\n",
    "            encoded_patches = layers.Reshape((h, h, c))(\n",
    "                encoded_patches\n",
    "            )  # (B, h, h, projection_dim)\n",
    "            encoded_patches = token_learner(\n",
    "                encoded_patches, token_learner_units\n",
    "            )  # (B, num_tokens, c)\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    # Initialize the AdamW optimizer.\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Define callbacks\n",
    "    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(test_ds)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 92ms/step - accuracy: 0.1208 - loss: 2.3749 - top-5-accuracy: 0.5525 - val_accuracy: 0.2679 - val_loss: 1.9079 - val_top-5-accuracy: 0.8177\n",
      "Epoch 2/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 90ms/step - accuracy: 0.2948 - loss: 1.8678 - top-5-accuracy: 0.8322 - val_accuracy: 0.3332 - val_loss: 1.7752 - val_top-5-accuracy: 0.8566\n",
      "Epoch 3/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.3831 - loss: 1.6548 - top-5-accuracy: 0.8842 - val_accuracy: 0.4350 - val_loss: 1.5397 - val_top-5-accuracy: 0.9088\n",
      "Epoch 4/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 92ms/step - accuracy: 0.4473 - loss: 1.5095 - top-5-accuracy: 0.9114 - val_accuracy: 0.4684 - val_loss: 1.4571 - val_top-5-accuracy: 0.9229\n",
      "Epoch 5/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 92ms/step - accuracy: 0.4768 - loss: 1.4403 - top-5-accuracy: 0.9257 - val_accuracy: 0.5019 - val_loss: 1.3739 - val_top-5-accuracy: 0.9319\n",
      "Epoch 6/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.5003 - loss: 1.3781 - top-5-accuracy: 0.9339 - val_accuracy: 0.5059 - val_loss: 1.3674 - val_top-5-accuracy: 0.9319\n",
      "Epoch 7/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 92ms/step - accuracy: 0.5127 - loss: 1.3407 - top-5-accuracy: 0.9383 - val_accuracy: 0.5261 - val_loss: 1.3131 - val_top-5-accuracy: 0.9416\n",
      "Epoch 8/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 92ms/step - accuracy: 0.5360 - loss: 1.2910 - top-5-accuracy: 0.9401 - val_accuracy: 0.5130 - val_loss: 1.3317 - val_top-5-accuracy: 0.9378\n",
      "Epoch 9/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.5404 - loss: 1.2658 - top-5-accuracy: 0.9465 - val_accuracy: 0.5474 - val_loss: 1.2715 - val_top-5-accuracy: 0.9399\n",
      "Epoch 10/10\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.5510 - loss: 1.2453 - top-5-accuracy: 0.9480 - val_accuracy: 0.5665 - val_loss: 1.2231 - val_top-5-accuracy: 0.9462\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.5653 - loss: 1.2253 - top-5-accuracy: 0.9462\n",
      "Test accuracy: 56.38%\n",
      "Test top 5 accuracy: 94.7%\n"
     ]
    }
   ],
   "source": [
    "vit_token_learner = create_vit_classifier()\n",
    "run_experiment(vit_token_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Beispiel: Laden eines Bildes\n",
    "def preprocess_image(image_path):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(32, 32))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, axis=0)  # Batch-Dimension hinzufügen\n",
    "    #img_array = img_array / 255.0  # Normalisierung\n",
    "    return img_array\n",
    "\n",
    "# Beispiel: Pfad zu Ihrem Bild\n",
    "image_path = \"Download.jpeg\"\n",
    "image = preprocess_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Class: 5\n"
     ]
    }
   ],
   "source": [
    "# Eine Vorhersage für ein Bild\n",
    "predictions = vit_token_learner.predict(image)\n",
    "\n",
    "# Wahrscheinlichkeiten in Kategorien umwandeln\n",
    "predicted_class = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "print(f\"Predicted Class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class Name: dog\n"
     ]
    }
   ],
   "source": [
    "# Mapping von Klassen-Indices zu Namen\n",
    "cifar10_classes = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Beispiel: Vorhersage in eine Klasse umwandeln\n",
    "predicted_class_name = cifar10_classes[predicted_class]\n",
    "print(f\"Predicted Class Name: {predicted_class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted Class Index: 1\n",
      "Predicted Class Name: automobile\n"
     ]
    }
   ],
   "source": [
    "# Beispiel für ein Bild\n",
    "predictions = vit_token_learner.predict(image)\n",
    "\n",
    "# Wahrscheinlichkeiten in Kategorien umwandeln\n",
    "predicted_class = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "\n",
    "# Mapping von Indizes zu Klassen\n",
    "cifar10_classes = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Umwandeln in Klassenname\n",
    "predicted_class_name = cifar10_classes[predicted_class]\n",
    "print(f\"Predicted Class Index: {predicted_class}\")\n",
    "print(f\"Predicted Class Name: {predicted_class_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
